---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# gdalcli: A Generative R Frontend for the GDAL (≥3.11) Unified CLI

[![Lifecycle: experimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://lifecycle.r-lib.org/articles/stages.html)

A modern R interface to GDAL's unified command-line interface (GDAL >=3.11). Provides a **lazy evaluation framework** for building and executing GDAL commands with composable, pipe-aware functions. Full support for native GDAL pipelines, GDALG format persistence, and advanced pipeline composition.

## Overview

`gdalcli` implements a frontend for GDAL's unified CLI with:

- **Auto-Generated Functions**: 80+ R wrapper functions automatically generated from GDAL's JSON API specification
- **Lazy Evaluation**: Build command specifications as objects (`gdal_job`), execute only when needed via `gdal_job_run()`
- **Pipe Composition**: Native R pipe (`|>`) support with S3 methods for adding options and environment variables
- **Native Pipeline Execution**: Direct execution via `gdal raster/vector pipeline` for efficient multi-step workflows
- **GDALG Format Support**: Save and load pipelines as JSON for persistence, sharing, and version control
- **Shell Script Generation**: Render pipelines as executable bash/zsh scripts (native or sequential modes)
- **VSI Streaming**: Support for `/vsistdin/` and `/vsistdout/` for file-less, memory-efficient workflows
- **Multiple Backends**: Use processx, gdalraster, or reticulate (Python) for backend processing

## Installation

```r
# Install from GitHub (when available)
# devtools::install_github("brownag/gdalcli")
library(gdalcli)
```

## Quick Examples

### Example 1: Building and Inspecting a Job

```{r}
library(gdalcli)

# Build a raster conversion job (lazy evaluation - nothing executes yet)
job <- gdal_raster_convert(
  input = "input.tif",
  output = "output.tif",
  output_format = "COG"
)

# Inspect the job object
print(job)
```

### Example 2: Adding Options to Jobs

```{r}
# Build a job with creation options and config options
job_with_options <- gdal_raster_convert(
  input = "input.tif",
  output = "output.tif",
  output_format = "COG"
) |>
  gdal_with_co("COMPRESS=LZW", "BLOCKXSIZE=256") |>
  gdal_with_config("GDAL_CACHEMAX=512")

print(job_with_options)
```

### Example 3: Rendering a Job as a Shell Command

```{r}
# Render the job as a shell command (executable but not run)
cmd <- render_gdal_pipeline(job)
cat("Command to execute:\n")
cat(cmd, "\n")
```

### Example 4: Building a Multi-Step Pipeline

```{r}
# Build a 3-step pipeline using native R piping
pipeline <- gdal_raster_reproject(
  input = "input.tif",
  dst_crs = "EPSG:32632"
) |>
  gdal_raster_scale(
    src_min = 0, src_max = 10000,
    dst_min = 0, dst_max = 255
  ) |>
  gdal_raster_convert(
    output = "output.tif",
    output_format = "COG"
  )

print(pipeline)
```

### Example 5: Rendering Pipeline to Native GDAL Format

```{r}
# Render as native GDAL pipeline (single efficient command)
native_cmd <- render_gdal_pipeline(pipeline, format = "native")
cat("Native GDAL pipeline command:\n")
cat(native_cmd, "\n")
```

### Example 6: Rendering Pipeline as Shell Script

```{r}
# Render as executable bash script (native mode)
native_script <- render_shell_script(pipeline, format = "native", shell = "bash")
cat("Bash script (native pipeline mode):\n")
cat(native_script, "\n")
```

### Example 7: Rendering Pipeline as Sequential Commands

```{r}
# Render as separate commands (safer for hybrid workflows)
seq_script <- render_shell_script(pipeline, format = "commands", shell = "bash")
cat("Bash script (sequential commands mode):\n")
cat(seq_script, "\n")
```

### Example 8: GDALG Format - Save Pipeline

```{r}
# Save pipeline to GDALG format
temp_file <- tempfile(fileext = ".gdalg.json")
gdal_save_pipeline(pipeline, temp_file)

# Display the saved GDALG JSON structure
cat("Saved GDALG file:\n")
cat(readLines(temp_file), sep = "\n")

# Clean up
unlink(temp_file)
```

### Example 9: GDALG Format - Round-Trip Testing

```{r}
# Demonstrate perfect round-trip fidelity
original_cmd <- render_gdal_pipeline(pipeline, format = "native")

# Save to GDALG and load back
temp_file <- tempfile(fileext = ".gdalg.json")
gdal_save_pipeline(pipeline, temp_file)
loaded_pipeline <- gdal_load_pipeline(temp_file)

# Render the loaded pipeline
loaded_cmd <- render_gdal_pipeline(loaded_pipeline, format = "native")

# Verify they're identical
cat("Original command:\n")
cat(original_cmd, "\n\n")

cat("Loaded command:\n")
cat(loaded_cmd, "\n\n")

cat("Commands identical:", identical(original_cmd, loaded_cmd), "\n")

# Clean up
unlink(temp_file)
```

### Example 10: Complex Pipeline with Configuration Options

```{r}
# Build a complex pipeline with cloud storage and config options
complex_pipeline <- gdal_raster_reproject(
  input = "/vsis3/sentinel-pds/input.tif",
  dst_crs = "EPSG:3857"
) |>
  gdal_raster_scale(src_min = 0, src_max = 10000, dst_min = 0, dst_max = 255) |>
  gdal_raster_convert(
    output = "/vsis3/my-bucket/output.tif",
    output_format = "COG"
  ) |>
  gdal_with_co("COMPRESS=LZW", "BLOCKXSIZE=256") |>
  gdal_with_config("AWS_REGION=us-west-2", "AWS_REQUEST_PAYER=requester")

# Render with config options included
native_with_config <- render_gdal_pipeline(complex_pipeline$pipeline, format = "native")
cat("Native pipeline with config options:\n")
cat(native_with_config, "\n")
```

## Pipeline Features

### Native GDAL Pipeline Execution

Execute multi-step workflows as a single native GDAL pipeline for maximum efficiency:

```{r}
# The native pipeline format runs all steps in a single command
# avoiding intermediate disk I/O for large datasets
pipeline <- gdal_raster_info(input = "input.tif") |>
  gdal_raster_reproject(dst_crs = "EPSG:32632") |>
  gdal_raster_convert(output = "output.tif")

# This would execute: gdal_job_run(pipeline, execution_mode = "native")
```

Native mode runs the entire pipeline in a single GDAL command, avoiding intermediate disk I/O for large datasets.

### GDALG Format: Save and Load Pipelines

Persist pipelines as JSON files for sharing and version control:

```{r, eval=FALSE}
# Save pipeline to GDALG format
pipeline <- gdal_raster_reproject(input = "in.tif", dst_crs = "EPSG:32632") |>
  gdal_raster_scale(src_min = 0, src_max = 100, dst_min = 0, dst_max = 255, output = "out.tif")

gdal_save_pipeline(pipeline, "workflow.gdalg.json")

# Load and execute later
loaded <- gdal_load_pipeline("workflow.gdalg.json")
gdal_job_run(loaded, execution_mode = "native")
```

GDALG provides perfect round-trip fidelity—all pipeline structure and arguments are preserved.

### Shell Script Generation

Generate executable shell scripts from pipelines:

```{r, eval=FALSE}
# Render as native GDAL pipeline script
script <- render_shell_script(pipeline, format = "native", shell = "bash")
writeLines(script, "process.sh")

# Or as separate sequential commands
script_seq <- render_shell_script(pipeline, format = "commands", shell = "bash")
```

### Configuration Options in Pipelines

Add GDAL configuration options to pipeline steps:

```{r}
pipeline_with_config <- gdal_raster_reproject(
  input = "in.tif",
  dst_crs = "EPSG:32632"
) |>
  gdal_with_config("OGR_SQL_DIALECT=SQLITE") |>
  gdal_raster_scale(
    src_min = 0, src_max = 100,
    dst_min = 0, dst_max = 255,
    output = "out.tif"
  )

# Config options are included in native pipeline rendering
render_gdal_pipeline(pipeline_with_config$pipeline, format = "native")
```

## GDAL Function Categories

The 80+ auto-generated functions are organized into logical categories:

### Raster Operations

Functions for working with raster datasets: `gdal_raster_*`

- **gdal_raster_convert** - Format conversion
- **gdal_raster_clip** - Spatial subsetting
- **gdal_raster_reproject** - Reprojection
- **gdal_raster_scale** - Value scaling
- And many more...

### Vector Operations

Functions for working with vector datasets: `gdal_vector_*`

- **gdal_vector_convert** - Format conversion
- **gdal_vector_info** - Dataset information
- **gdal_vector_reproject** - Reprojection
- And more...

### Multidimensional (MDim)

Functions for multidimensional data: `gdal_mdim_*`

- **gdal_mdim_convert** - Dimension conversion
- **gdal_mdim_info** - Dimension information

### VSI (Virtual File System)

Functions for cloud storage and remote access: `gdal_vsi_*`

- Support for S3, Azure, GCS, OSS, Swift with automatic credential handling

Use `gdal_gdal(drivers = TRUE)` to list all available drivers in your GDAL installation.

## System Requirements

- **R** >= 4.1
- **GDAL** >= 3.11 (required for unified CLI)
- **Dependencies**:
  - `processx` (>=3.8.0) - Robust subprocess management
  - `yyjsonr` (>=0.1.0) - Fast, memory-efficient JSON handling
  - `rlang` (>=1.0.0) - Error handling and programming utilities
  - `cli` (>=3.0.0) - User-friendly terminal messages
  - `digest` (>=0.6.0) - Cryptographic hashing

## Documentation

- **Getting Started**: `?gdal_vector_convert` - Example auto-generated function
- **Authentication**: `?gdal_auth_s3`, `?gdal_auth_azure`, etc. - Set up credentials
- **Lazy Evaluation**: `?gdal_job` - Understand the job specification system
- **Execution**: `?gdal_job_run` - Run GDAL commands
- **Composition**: `?gdal_with_co`, `?gdal_with_env` - Modify jobs with modifiers

## Architecture & Design

### Three-Layer Architecture

`gdalcli` separates concerns into three layers:

1. **Frontend Layer** (User-facing R API)
   - Auto-generated functions like `gdal_vector_convert()`, `gdal_raster_reproject()`, etc.
   - Composable modifiers: `gdal_with_co()`, `gdal_with_env()`, `gdal_with_config()`, etc.
   - S3 methods for extensibility
   - Lazy `gdal_job` specification objects
   - Native pipe (`|>`) support for fluent composition

2. **Pipeline Layer** (Advanced Workflows)
   - Automatic pipeline building through chained piping
   - Native GDAL pipeline execution (`gdal raster/vector pipeline`)
   - GDALG format serialization with `gdal_save_pipeline()` and `gdal_load_pipeline()`
   - Shell script rendering for persistence and sharing
   - Configuration option aggregation and propagation
   - Sequential vs. native execution modes

3. **Engine Layer** (Command Execution)
   - `gdal_job_run()` executes individual jobs or entire pipelines
   - Uses processx for robust subprocess management
   - Handles environment variable injection
   - Supports input/output streaming (`/vsistdin/`, `/vsistdout/`)
   - Multiple backend options (processx, gdalraster, reticulate)

### Lazy Evaluation

Commands are built as specifications (`gdal_job` objects) and only executed when passed to `gdal_job_run()`:

```{r}
# This doesn't execute anything - just builds a specification
job <- gdal_vector_convert(
  input = "data.shp",
  output = "data.gpkg"
)

# Inspect the job before running
print(job)

# Render to see the command that would be executed
render_gdal_pipeline(job)
```

### S3-Based Composition

All modifiers are S3 generics that accept and return `gdal_job` objects, enabling composable workflows:

```{r}
# Each step returns a modified gdal_job
pipeline <- gdal_raster_convert(input = "in.tif", output = "out.tif") |>
  gdal_with_co("COMPRESS=DEFLATE") |>
  gdal_with_config("GDAL_CACHEMAX=512")

# Inspect at any point
print(pipeline)
```

### Pipeline Composition and Execution Modes

Pipelines are automatically created when chaining GDAL operations with the native R pipe:

```{r}
# Build a multi-step pipeline by chaining operations
pipeline <- gdal_raster_info(input = "input.tif") |>
  gdal_raster_reproject(dst_crs = "EPSG:32632") |>
  gdal_raster_convert(output = "output.tif")

# Render as sequential commands (jobs run separately)
seq_render <- render_gdal_pipeline(pipeline$pipeline, format = "shell_chain")
cat("Sequential:", seq_render, "\n\n")

# Render as native GDAL pipeline (single efficient command)
native_render <- render_gdal_pipeline(pipeline$pipeline, format = "native")
cat("Native:", native_render, "\n")
```

**Sequential Execution** (default):

- Each job runs as a separate GDAL command
- Safer for hybrid workflows mixing pipeline and non-pipeline operations
- Intermediate results written to disk

**Native Execution**:

- Entire pipeline runs as single `gdal raster/vector pipeline` command
- More efficient for large datasets (avoids intermediate I/O)
- Direct data flow between pipeline steps

### GDALG Format: Pipeline Persistence

Pipelines can be saved and loaded for persistence and sharing:

```{r, eval=FALSE}
# Save pipeline as GDALG (JSON format)
gdal_save_pipeline(pipeline, "workflow.gdalg.json")

# Load pipeline later
loaded <- gdal_load_pipeline("workflow.gdalg.json")

# Pipelines maintain perfect round-trip fidelity
# All structure, arguments, and metadata are preserved
```

GDALG files can be:

- Version controlled in git repositories
- Shared with team members
- Executed by other GDAL-compatible tools
- Edited manually for advanced workflows

## Security Considerations

1. **Credentials in .Renviron, not in code** - Never pass secrets as function arguments. Use `.Renviron` or external secret managers:

   ```env
   # ~/.Renviron or project .Renviron
   AWS_ACCESS_KEY_ID=your_access_key
   AWS_SECRET_ACCESS_KEY=your_secret
   ```

2. **Environment-variable-only design** - Auth helpers read from environment, never from function arguments. This prevents accidental commits of credentials.

3. **Don't save R sessions** - Avoid saving `.RData` files or R session history containing credentials.

4. **Use temporary/rotatable credentials** - When possible, use temporary credentials (AWS STS, Azure Managed Identity, GCS service accounts) instead of long-lived secrets.

5. **Rotate credentials regularly** - Follow your organization's credential rotation policies.

6. **Use external secret managers** - For production, consider:
   - HashiCorp Vault
   - AWS Secrets Manager
   - Azure Key Vault
   - Google Cloud Secret Manager
   - 1Password, Bitwarden, or other password managers with environment variable integration

## Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/my-feature`)
3. Commit your changes (`git commit -am 'Add my feature'`)
4. Push to the branch (`git push origin feature/my-feature`)
5. Open a Pull Request

## License

MIT License - see LICENSE file for details

## References

- **GDAL Unified CLI (≥3.11)**: [https://gdal.org/programs/index.html](https://gdal.org/programs/index.html)
- **GDAL Pipeline Documentation**: [https://gdal.org/programs/gdalinfo.html](https://gdal.org/programs/gdalinfo.html) (see `gdal raster pipeline`, `gdal vector pipeline`)
- **GDAL JSON Usage API**: [https://gdal.org/development/rfc/rfc90.html](https://gdal.org/development/rfc/rfc90.html)
- **GDAL Virtual File Systems**: [https://gdal.org/user/virtual_file_systems.html](https://gdal.org/user/virtual_file_systems.html)
- **GDAL Configuration Options**: [https://gdal.org/user/configoptions.html](https://gdal.org/user/configoptions.html)
- **Lazy Evaluation in R**: Inspired by dbplyr, rlang, and tidyverse design patterns
- **S3 Object System**: [https://adv-r.hadley.nz/s3.html](https://adv-r.hadley.nz/s3.html)
- **processx Package**: [https://processx.r-lib.org/](https://processx.r-lib.org/)
- **yyjsonr Package**: [https://cran.r-project.org/web/packages/yyjsonr/index.html](https://cran.r-project.org/web/packages/yyjsonr/index.html)

## Acknowledgments

This package is built on GDAL's unified CLI (≥3.11) and the GDAL development team's work.
